= Tutorial: Writing your First Commodore Component

This tutorial will guide you through the required steps to create your first Commodore component.

== Introduction

Commodore components allow you to customize and extend the number and variety of tasks that you can perform with https://syn.tools/[Project Syn]. Using Commodore components, you can customize and apply changes to all of your clusters, ensuring conformity, coherence, alignment, and drastically reducing the workload for DevOps engineers.

There are many https://github.com/topics/commodore-component[Commodore components] already published; they perform various tasks, all related to the maintenance of Kubernetes clusters of many different kinds, including https://www.openshift.com/[OpenShift] clusters.

WARNING: This tutorial has been prepared with version v0.2.0 of Commodore. Project Syn is moving fast, and we will update this tutorial accordingly as new features and capabilities are made available by the development team.

In this tutorial, we are going to do the following:

. First, we are going to launch two lightweight Kubernetes clusters, one Minikube and one Rancher K3s, and then we're going to federate them under the same tenant.
. Then, we are going to deploy a workload in both clusters at the same time.
. Finally, we are going to wrap up, not without first cleaning up all the different pieces of this experiment.

This tutorial should give you a good idea of how all the different pieces of Project Syn work together, and how Commodore components fit in the picture.

== Requirements

As for the required knowledge bits and pieces:

* You should understand https://syn.tools/syn/about/features.html[what Project Syn is], and its https://syn.tools/syn/about/architecture.html[architecture].
* You should have already followed the https://docs.syn.tools/syn/getting-started.html[Getting Started with Project Syn] tutorial, and be familiar with common Project Syn tasks.

WARNING: This tutorial *was built and tested on a Linux system*; it should, however, be easily ported to macOS, since most of the tools used are cross-platform.

To follow this tutorial you will need quite an array of software in your system:

* https://www.docker.com/[Docker] version 19, or https://podman.io/[podman] version 2.0
* https://kubernetes.io/docs/tasks/tools/install-minikube/[Minikube] version 1.12
* https://k3d.io/[K3d] version 3.0
* A https://about.gitlab.com/[GitLab] account with your https://gitlab.com/profile/keys[SSH key configured]. You can use a private GitLab instance, if you have one nearby.
* A https://github.com/[GitHub] account with your https://github.com/settings/keys[SSH key configured].
* The following commands must be installed and available in the path: https://curl.haxx.se/[curl], https://stedolan.github.io/jq/[jq], https://github.com/mikefarah/yq[yq], https://k9scli.io/[k9s] `ssh-keyscan`, and `base64`
* As an editor, we recommend using https://code.visualstudio.com/[Visual Studio Code] with the following extensions:
** https://github.com/hanseltime/vscode-jsonnet[vscode-jsonnet]
** https://marketplace.visualstudio.com/items?itemName=ahebrank.yaml2json[YAML to JSON extension]


This tutorial project contains a `requirements.sh` script which outputs all of your currently installed versions of the aforementioned software packages.

IMPORTANT: The specified version numbers are the minimum required. Also, please make sure that both your GitLab and GitHub accounts must have SSH keys configured.

== Steps

Are you ready? The let's get started!

=== 1. Preparations

Make sure to follow these three steps *before* starting the tutorial:

. Check all required tools by running the `./requirements.sh` script.
. In GitHub, fork the https://github.com/projectsyn/commodore-defaults[commodore-defaults] project to your own personal account.
. And in GitLab, create a https://gitlab.com/profile/personal_access_tokens[Personal Access Token] with API access.

CAUTION: Remember to save your token somewhere, since it will only appear once!

=== 2. Install Lieutenant Operator and API on Minikube

The https://syn.tools/lieutenant-operator/[Lieutenant Operator] and its https://syn.tools/lieutenant-api/home.html[API] are the major elements of a Project Syn installation. They provide a complete catalog of information about all of your clusters, and allow developers to operate on those clusters through a unique API.

In this tutorial we are going to install Lieutenant in the same Minikube cluster that we will use later on; this is usually not the case, and Lieutenant and its API are usually installed separately. Doing so will, however, simplify our setup greatly.

First you need to set a few environment variables:

[source,bash]
----
$ export GITLAB_TOKEN="TOKEN_CREATED_IN_PREVIOUS_STEP"
$ export GITLAB_ENDPOINT=gitlab.com
$ export GITLAB_USERNAME=your_gitlab_username
$ export GITHUB_USERNAME=your_github_username
----

To install Lieutenant Operator and API on Minikube, we are going to execute the `./1_lieutenant.sh` script. This script mimicks the steps explained in the https://docs.syn.tools/syn/tutorials/getting-started.html["Getting Started"] tutorial, so we will not go through them here. The script is quite self-explanatory, though:

[source,bash]
----
# â€¦
include::1_lieutenant.sh[tag=demo]
# â€¦
----

This script will launch a new Minikube instance, will apply all the required `kubectl` commands, and wait for the cluster to be in the expected state before continuing.

Once the script has run, you will find two new private repositories in your GitLab account: one named `cluster-gitops1` and another named `mytenant`. Project Syn tools use these repositories to perform https://www.gitops.tech/["GitOps"] operations on all the clusters.

.GitLab account after installation of Lieutenant
image::gitlab_01.png[]

=== 3. Install Steward on Minikube

Now that Lieutenant is installed we can install https://docs.syn.tools/steward/[Steward] on our cluster. This tool is in charge of watching the GitLab and GitHub repositories, and triggers the GitOps operations required to keep each cluster up-to-date.

We can easily install Steward on Minikube using the `./3_steward_on_minikube.sh` script.

=== 4. Install Steward on K3s

Now it is time to spin our second cluster, this time using K3s, itself managed using K3d. Again, a couple of scripts will make our life easier.

First we need to get some variables from our environment:

[source,bash]
----
$ source ./env.sh
----

And now we can run a script that will not only spin a new k3s cluster, but will also install Steward in it:

[source,bash]
----
$ ./5_steward_on_k3s.sh
----

And now we are ready: we have two working Kubernetes clusters, one Minikube and one k3s, and both are assigned to the same tenant. To verify this, we ask now the Lieutenant API, installed in our Minikube cluster:

[source,bash]
----
$ kubectl config use-context minikube
Switched to context "minikube".

$ kubectl get clusters -n lieutenant
NAME                  DISPLAY NAME       TENANT                AGE
c-damp-dew-2385       K3s cluster        t-muddy-sunset-5530   78s
c-small-cherry-9211   Minikube cluster   t-muddy-sunset-5530   48m

$ kubectl get tenants -n lieutenant
NAME                  DISPLAY NAME      AGE
t-muddy-sunset-5530   Tutorial Tenant   49m
----

Of course, in your case the NAME column will show different, random values. But we can see both clusters and the tenant object, all happily residing inside the realm of our Project Syn installation.

NOTE: For more information, check out the https://syn.tools/lieutenant-operator/explanation/design.html[diagram] in the Project Syn website.

=== 5. Creating a Commodore Component

This section will dive into the actual work of creating Commodore components.

==== Checking out Commodore

We need to clone the official Commodore project from GitHub first, and inside of that clone of the Commodore source code, we will create a few folders, and a new file called `.env`:

[source,bash]
----
$ git clone git@github.com:projectsyn/commodore.git
$ cd commodore
$ mkdir -p catalog inventory dependencies compiled
$ touch .env
----

==== Using the Commodore Docker Image

We are going to use now the `commodore` tool to generate our component. This tool is written in Python and requires quite a few dependencies; to make our lives easier, we are instead going to use the corresponding https://hub.docker.com/r/projectsyn/commodore[container image available in Docker Hub]. This is why it is strongly recommended that you define the following function in your environment, which will make `commodore` much easier to use:

[source,bash]
----
$ which commodore
commodore not found

$ source ./commodore.sh

$ which commodore
commodore () {
	docker run --env-file=.env --interactive=true --tty --rm --user="$(id -u)" --volume ~/.ssh:/app/.ssh:ro --volume "$PWD"/compiled/:/app/compiled/ --volume "$PWD"/catalog/:/app/catalog --volume "$PWD"/dependencies/:/app/dependencies/ --volume "$PWD"/inventory/:/app/inventory/ --volume ~/.gitconfig:/app/.gitconfig:ro projectsyn/commodore:v0.2.0 "$*"
}
----

We also need to define the contents of the `.env` file, used by Docker to set environment variables required by the `commodore` command. We can do this in a snap with this command, and in my machine this is the result:

[source,bash]
----
$ source ./env.sh -docker > .env

$ cat .env
# URL of Lieutenant API
COMMODORE_API_URL=http://lieutenant.172.17.0.3.nip.io:32506
# Lieutenant API token
COMMODORE_API_TOKEN=â€¦â€¦â€¦
# Base URL for global Git repositories
COMMODORE_GLOBAL_GIT_BASE=ssh://git@github.com/akosma
----

==== Compiling the Commodore Catalog

The first step to create a Commodore component is what is usually referred to as "compiling the catalog." This catalog provides a unique reference point of information about all the configuration and workloads affecting a single cluster.

Now we are ready to compile our catalog. Every time you call the `commodore` tool you will have to enter the password of your SSH key.

[source,bash]
----
$ commodore catalog compile --push $MINIKUBE_CLUSTER_ID
$ commodore catalog compile --push $K3S_CLUSTER_ID
----

NOTE: We must repeat this operation for each cluster.

==== Creating a New Commodore Component

With our catalog ready to use, we can create a new component. In this case, we want to create a component that automatically deploys a very simple application called the https://hub.docker.com/r/vshn/fortune-cookie-service["Fortune Cookie Service"] to our clusters. This application, written in Python, returns a funny "fortune cookie" message in the console.

Let us create the component first:

[source,bash]
----
$ commodore component new fortune
Agent pid 9
Enter passphrase for /app/.ssh/id_ed25519: 
Identity added: /app/.ssh/id_ed25519 (adrian.kosmaczewski@vshn.ch)
Adding component fortune...
 > Installing component
Component fortune successfully added ðŸŽ‰
----

The new component is located now in the `dependencies/fortune` folder. We can edit it using Visual Studio Code:

[source,bash]
----
$ code dependencies/fortune
----

.Editing the Fortune component
image::vscode_01.png[]

Commodore components are written using https://jsonnet.org/[Jsonnet] (pronounced "jay-sonnet"), a data templating language for app and tool developers. If you have never written Jsonnet before, do not worry; just know that all valid JSON files are valid Jsonnet files, and that Jsonnet provides useful extensions, such as variables and functions, and various export formats, which make it easy to write very complex JSON applications with it.

In Visual Studio Code, open the `component/main.jsonnet` file, and replace its text with the following:

[source,jsonnet]
----
include::assets/code/main.jsonnet[]
----

<1> This Jsonnet file provides the definition of a complete Kubernetes deployment. However, instead of being specified in YAML, this file specifies it in Jsonnet format. After the `import` statements we define a few variables.
<2> This object defines a new Kubernetes namespace.
<3> This object defines a Kubernetes service.
<4> Finally, this object specifies a rather standard Kubernetes deployment.
<5> This variable holds the container image that will be deployed to our clusters.

The https://github.com/hanseltime/vscode-jsonnet[vscode-jsonnet] extension for Visual Studio Code has a nifty "live preview" feature; the picture below shows how Jsonnet outputs the classic "YAML" format that any Kubernetes cluster might expect.

.Previewing the Jsonnet compilation
image::vscode_02.png[]

[TIP]
.Configuring the Jsonnet preview pane
====
The Jsonnet extension for Visual Studio Code requires some values in `settings.json` in order to work properly:

[source,json]
----
"jsonnet.executablePath": "/home/username/.local/bin/jsonnet",
"jsonnet.libPaths": [
    "/home/username/commodore/commodore/",
    "/home/username/commodore/dependencies/",
    "/home/username/.cache/pypoetry/virtualenvs/commodore-jtwx3-r8-py3.7/lib/python3.7/site-packages/kapitan"
]
----

The value of the third entry of the `jsonnet.libPaths` property can be found using the `poetry env info` command.
====

[TIP]
.Creating Jsonnet files from YAML
====
To create your own Jsonnet files, you can just translate them from plain YAML using either the https://github.com/mikefarah/yq[yq] tool, or the https://marketplace.visualstudio.com/items?itemName=ahebrank.yaml2json[YAML to JSON extension] for Visual Studio Code.
====

Open the `component/app.jsonnet` file and replace its contents with the following:

[source,jsonnet]
----
include::assets/code/app.jsonnet[]
----

<1> This `secrets=false` parameter is required. This is due to a problem that ArgoCD has with the Kapitan plugin at the time of this writing, which prevents it from loading properly at the moment. Since we do not require secrets for this tutorial, we can safely disable it for the moment.

==== Compiling the Component

Now that we have written our component, let's compile it to see if it's generating what we need:

[source,bash]
----
$ commodore component compile dependencies/fortune          
Agent pid 9
Enter passphrase for /app/.ssh/id_ed25519: 
Identity added: /app/.ssh/id_ed25519 (adrian.kosmaczewski@vshn.ch)
Compile component fortune...
Updating Jsonnet libraries...
Compiling catalog...
 > Component compiled to /app/compiled/test

$ ls compiled/test/fortune 
.  ..  deployment.yaml  namespace.yaml  service.yaml

$ cat compiled/test/fortune/namespace.yaml        
apiVersion: v1
kind: Namespace
metadata:
  labels:
    name: syn-tutorial
  name: syn-tutorial
----

Our component compiles! Let us create a new project in GitHub and push our code:

image::github_new_proj.png[]

[source,bash]
----
$ cd dependencies/fortune
$ git add .
$ git commit -m "Added code to component"
$ git remote remove origin
$ git remote add origin git@github.com:akosma/component-fortune.git
$ git push --set-upstream origin master
$ cd ../../
----

==== Adding the Component to our Clusters

Now that we have a component that compiles, and that we have pushed it to GitHub, we want to deploy it to our clusters.

We need to edit two files for that:

. `cd inventory/classes/global`
** This folder is a clone of the fork of the `commodore-defaults` project.
. Edit `commodore.yml` and add this at the end:
+
[source,bash]
----
- name: fortune
  url: https://github.com/akosma/component-fortune.git
----
. `git commit -a -m "Added component"`
. `git push`
. `cd ../[$TENANT_ID]`
** This folder is a clone of the `mytenant` project in your GitLab account.
. Edit `/common.yml` and add this text:
+
[source,bash]
----
classes:
- components.fortune
----
. `git commit -a -m "Added component"`
. `git push`
. `cd ../../../`

NOTE: Since we edited the `common.yml` file, the changes will propagate to all clusters.

==== Final Step

We're ready! Now we just have to recompile the catalog and push the changes.

[source,bash]
----
$ commodore catalog compile --push $MINIKUBE_CLUSTER_ID
Agent pid 9
Enter passphrase for /app/.ssh/id_ed25519:
Identity added: /app/.ssh/id_ed25519 (adrian.kosmaczewski@vshn.ch)
Cleaning working tree
Updating global config...
Updating customer config...
Discovering components...
Fetching components...
Updating Kapitan target...
Updating cluster catalog...
Updating Jsonnet libraries...
Cleaning catalog repository...
Updating Kapitan secret references...
Compiling catalog...
Postprocessing...
   > compiled/cluster/metrics-server/01_helmchart/metrics-server/templates/00_namespace.yaml doesn't exist, creating...
Updating catalog repository...
 > Changes:
     Added file manifests/apps/fortune.yaml
     Added file manifests/fortune/deployment.yaml
     Added file manifests/fortune/namespace.yaml
     Added file manifests/fortune/service.yaml
 > Commiting changes...
 > Pushing catalog to remote...
Catalog compiled! ðŸŽ‰
----

Since ArgoCD, itself managed by Steward, is watching these repositories, it will pick up the new state and seamlessly deploy it across our two clusters in a few minutes.

== Conclusion

I hope this tutorial has given you a good idea of what Commodore components are, and how they can help you in your DevOps workflow.

Commodore components can be used to deploy monitoring tools, to change the state of lots of clusters at once, to perform backup operations, to modify network policies, and many other tasks that otherwise would have to be performed manually.

Creating Commodore components at the moment is a long task, but the rewards are high in terms of the economies of scale DevOps reach when managing multi-cluster architectures.

If you create a new open source Commodore component, add the https://github.com/topics/commodore-component[`commodore-component` tag] on your GitHub project and let us know about it! We will be thrilled to check it out and maybe even contribute back to your effort.
